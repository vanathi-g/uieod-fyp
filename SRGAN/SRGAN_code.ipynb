{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Color Correction GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOIHrCCqHnY_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, Activation, BatchNormalization, UpSampling2D, Input, LeakyReLU, Add, Dense\n",
        "\n",
        "import numpy as np\n",
        "from numpy import cov, trace, iscomplexobj\n",
        "from scipy.linalg import sqrtm\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UIEB dataset is available here\n",
        "!git clone https://github.com/vanathi-g/fyp-datasets.git\n",
        "!cd fyp-datasets/"
      ],
      "metadata": {
        "id": "nnoVAGwYHrLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "v0MXRXJ0Cye8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = '/content/gdrive/MyDrive/SRGAN_weights/' # Path to model weights save folder in Google Drive\n",
        "model_save_filepath = '/content/gdrive/MyDrive/SRGAN_models/' # Path to model save folder"
      ],
      "metadata": {
        "id": "PoOvlpJiDmgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Preprocessing and Display Functions\n",
        "---\n",
        "### Image dimensions:\n",
        "min width, height = 225, 159<br>\n",
        "max width, height = 2180, 1450"
      ],
      "metadata": {
        "id": "UsiU13wNKrCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image preprocessing\n",
        "def load_all_images(data_dir, image_shape):\n",
        "  all_images = glob.glob(data_dir)\n",
        "  original_images = []\n",
        "  enhanced_images = []\n",
        "  width, height = image_shape\n",
        "\n",
        "  for img in all_images:\n",
        "    img1 = Image.open(img)\n",
        "    img2 = Image.open(\"/content/fyp-datasets/reference-890/\"+img[30:])\n",
        "\n",
        "    if(img1.size[0] < width or img1.size[1] < height):\n",
        "      continue\n",
        "\n",
        "    # Cropping image to extract max number of patches of specified shape from each image\n",
        "\n",
        "    start = [0, 0]\n",
        "    while True:\n",
        "      crop_coords = (start[0], start[1], start[0] + width, start[1] + height)\n",
        "      cropped1 = img1.crop(crop_coords)\n",
        "      cropped2 = img2.crop(crop_coords)\n",
        "\n",
        "      original = np.array(cropped1)\n",
        "      enhanced = np.array(cropped2)\n",
        "\n",
        "      original_images.append(original)\n",
        "      enhanced_images.append(enhanced)\n",
        "\n",
        "      start[0] += width\n",
        "      if(img1.size[0] - start[0] < width):\n",
        "        start[0] = 0\n",
        "        start[1] += height\n",
        "      if(img1.size[1] - start[1] < height):\n",
        "        break\n",
        "\n",
        "  return np.array(enhanced_images), np.array(original_images)"
      ],
      "metadata": {
        "id": "Rr0TpfC2gsmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_random_batch(original_images, enhanced_images, batch_size):\n",
        "  selected_ind = np.random.choice(range(0, len(original_images)), batch_size)\n",
        "  selected_original = np.array([original_images[x] for x in selected_ind])\n",
        "  selected_enhanced = np.array([enhanced_images[x] for x in selected_ind])\n",
        "  return selected_original, selected_enhanced"
      ],
      "metadata": {
        "id": "EFR1PtdUq4YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_test_images(data_dir, batch_size, image_shape):\n",
        "  all_images = glob.glob(data_dir)\n",
        "  images_batch = np.random.choice(all_images, size=batch_size)\n",
        "  images = []\n",
        "\n",
        "  for img in images_batch:\n",
        "    img1 = Image.open(img)\n",
        "    img1 = img1.crop((0, 0, image_shape[0], image_shape[1]))\n",
        "    img1_test = np.array(img1)\n",
        "    images.append(img1_test)\n",
        "  return np.array(images)"
      ],
      "metadata": {
        "id": "0TVfObQI4h-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_images(original_img, enhanced_img, generated_img):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(1, 3, 1)\n",
        "    original_img = (original_img + 1)/2\n",
        "    ax.imshow(original_img)\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(\"Original Image\")\n",
        "\n",
        "    ax = fig.add_subplot(1, 3, 2)\n",
        "    enhanced_img = (enhanced_img + 1)/2\n",
        "    ax.imshow(enhanced_img)\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(\"Enhanced Image\")\n",
        "\n",
        "    ax = fig.add_subplot(1, 3, 3)\n",
        "    generated_img = (generated_img + 1)/2\n",
        "    ax.imshow(generated_img)\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(\"Generated Image\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "doJNOx8QKTRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(loss_arr, intervals, label):\n",
        "  plt.plot(intervals, loss_arr, 'g', label=label)\n",
        "  plt.title('Visualizing ' + label)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "r48sJMoRiK9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the Model\n",
        "---"
      ],
      "metadata": {
        "id": "jqJiCcyNKv9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_block(x):\n",
        "  filters = [64, 64]\n",
        "  kernel_size = 3\n",
        "  strides = 1\n",
        "  padding = \"same\"\n",
        "  momentum = 0.8\n",
        "  activation = \"relu\"\n",
        "\n",
        "  res = Conv2D(filters=filters[0], kernel_size=kernel_size, strides=strides, padding=padding)(x)\n",
        "  res = Activation(activation=activation)(res)\n",
        "  res = BatchNormalization(momentum=momentum)(res)\n",
        "  res = Conv2D(filters=filters[1], kernel_size=kernel_size, strides=strides, padding=padding)(res)\n",
        "  res = BatchNormalization(momentum=momentum)(res)\n",
        "  # Elementwise sum\n",
        "  res = Add()([res, x])\n",
        "  return res"
      ],
      "metadata": {
        "id": "Wi80mTkSLoR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator():\n",
        "  residual_blocks = 8\n",
        "  momentum = 0.8\n",
        "  input_shape = (None, None, 3)\n",
        "\n",
        "  # Input Layer of the generator network\n",
        "  input_layer = Input(shape=input_shape)\n",
        "\n",
        "  # Add the pre-residual block\n",
        "  gen1 = Conv2D(filters=64, kernel_size=9, strides=1, padding='same',\n",
        "  activation='relu')(input_layer)\n",
        "\n",
        "  # Add 8 residual blocks\n",
        "  res = residual_block(gen1)\n",
        "  for i in range(residual_blocks - 1):\n",
        "    res = residual_block(res)\n",
        "\n",
        "  # Add the post-residual block\n",
        "  gen2 = Conv2D(filters=64, kernel_size=3, strides=1, padding='same')(res)\n",
        "  gen2 = BatchNormalization(momentum=momentum)(gen2)\n",
        "\n",
        "  # Take the sum of the output from the pre-residual block(gen1) and the post-residual block(gen2)\n",
        "  # Skip connection\n",
        "  gen3 = Add()([gen2, gen1])\n",
        "\n",
        "  # Add an upsampling block\n",
        "  gen4 = UpSampling2D(size=2)(gen3)\n",
        "  gen4 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same')(gen4)\n",
        "  gen4 = Activation('relu')(gen4)\n",
        "\n",
        "  # Add another upsampling block\n",
        "  gen5 = UpSampling2D(size=2)(gen4)\n",
        "  gen5 = Conv2D(filters=256, kernel_size=3, strides=1,\n",
        "  padding='same')(gen5)\n",
        "  gen5 = Activation('relu')(gen5)\n",
        "\n",
        "  # Output convolution layer\n",
        "  gen6 = Conv2D(filters=3, kernel_size=9, strides=1, padding='same')(gen3)\n",
        "  output = Activation('tanh')(gen6)\n",
        "\n",
        "  # Keras model\n",
        "  model = Model(inputs=[input_layer], outputs=[output], name='generator')\n",
        "  return model"
      ],
      "metadata": {
        "id": "8U40172AKvp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator():\n",
        "  leakyrelu_alpha = 0.2\n",
        "  momentum = 0.8\n",
        "\n",
        "  input_shape = (256, 256, 3)\n",
        "  input_layer = Input(shape=input_shape)\n",
        "\n",
        "  # Add the first convolution block\n",
        "  dis1 = Conv2D(filters=64, kernel_size=3, strides=1, padding='same')(input_layer)\n",
        "  dis1 = LeakyReLU(alpha=leakyrelu_alpha)(dis1)\n",
        "\n",
        "  # Add the 2nd convolution block\n",
        "  dis2 = Conv2D(filters=64, kernel_size=3, strides=2, padding='same')(dis1)\n",
        "  dis2 = LeakyReLU(alpha=leakyrelu_alpha)(dis2)\n",
        "  dis2 = BatchNormalization(momentum=momentum)(dis2)\n",
        "\n",
        "  # Add the third convolution block\n",
        "  dis3 = Conv2D(filters=128, kernel_size=3, strides=1, padding='same')(dis2)\n",
        "  dis3 = LeakyReLU(alpha=leakyrelu_alpha)(dis3)\n",
        "  dis3 = BatchNormalization(momentum=momentum)(dis3)\n",
        "\n",
        "  # Add the fourth convolution block\n",
        "  dis4 = Conv2D(filters=128, kernel_size=3, strides=2, padding='same')(dis3)\n",
        "  dis4 = LeakyReLU(alpha=leakyrelu_alpha)(dis4)\n",
        "  dis4 = BatchNormalization(momentum=0.8)(dis4)\n",
        "\n",
        "  # Add the fifth convolution block\n",
        "  dis5 = Conv2D(256, kernel_size=3, strides=1, padding='same')(dis4)\n",
        "  dis5 = LeakyReLU(alpha=leakyrelu_alpha)(dis5)\n",
        "  dis5 = BatchNormalization(momentum=momentum)(dis5)\n",
        "\n",
        "  # Add the sixth convolution block\n",
        "  dis6 = Conv2D(filters=256, kernel_size=3, strides=2, padding='same')(dis5)\n",
        "  dis6 = LeakyReLU(alpha=leakyrelu_alpha)(dis6)\n",
        "  dis6 = BatchNormalization(momentum=momentum)(dis6)\n",
        "\n",
        "  # Add the seventh convolution block\n",
        "  dis7 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same')(dis6)\n",
        "  dis7 = LeakyReLU(alpha=leakyrelu_alpha)(dis7)\n",
        "  dis7 = BatchNormalization(momentum=momentum)(dis7)\n",
        "\n",
        "  # Add the eight convolution block\n",
        "  dis8 = Conv2D(filters=256, kernel_size=3, strides=2, padding='same')(dis7)\n",
        "  dis8 = LeakyReLU(alpha=leakyrelu_alpha)(dis8)\n",
        "  dis8 = BatchNormalization(momentum=momentum)(dis8)\n",
        "\n",
        "  # Add a dense layer\n",
        "  dis9 = Dense(units=256)(dis8)\n",
        "  dis9 = LeakyReLU(alpha=0.2)(dis9)\n",
        "  \n",
        "  # Last dense layer - for classification\n",
        "  output = Dense(units=1, activation='sigmoid')(dis9)\n",
        "  model = Model(inputs=[input_layer], outputs=[output], name='discriminator')\n",
        "  return model"
      ],
      "metadata": {
        "id": "_3-Eue0kCZHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vgg():\n",
        "  # Load pre-trained VGG19 model trained on 'Imagenet' dataset\n",
        "  vgg = tf.keras.applications.VGG19(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
        "  vgg.trainable = False\n",
        "\n",
        "  outputs = [vgg.layers[10].output]\n",
        "\n",
        "  # Create a Keras model\n",
        "  model = Model([vgg.input], outputs) #TODO: Try changing no. of layers\n",
        "  return model"
      ],
      "metadata": {
        "id": "A3CvTvy58zDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation metric - Frechet Inception Distance (FID)\n",
        "---"
      ],
      "metadata": {
        "id": "9xb_vG6elvD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_fid(model, images1, images2):\n",
        "\t# calculate activations\n",
        "\tact1 = model.predict(images1)\n",
        "\tact2 = model.predict(images2)\n",
        "\t# calculate mean and covariance statistics\n",
        "\tmu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
        "\tmu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
        "\t# calculate sum squared difference between means\n",
        "\tssdiff = np.sum((mu1 - mu2)**2.0)\n",
        "\t# calculate sqrt of product between cov\n",
        "\tcovmean = sqrtm(sigma1.dot(sigma2))\n",
        "\t# check and correct imaginary numbers from sqrt\n",
        "\tif iscomplexobj(covmean):\n",
        "\t\tcovmean = covmean.real\n",
        "\t# calculate score\n",
        "\tfid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "\treturn fid"
      ],
      "metadata": {
        "id": "fr-nDCbx45DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model\n",
        "---"
      ],
      "metadata": {
        "id": "yqrEkvXDKwi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a checkpoint (to model weights save, model save locations) while training\n",
        "def take_checkpoint(generator, discriminator, checkpoint_filepath, epoch):\n",
        "  generator.save(model_save_filepath + \"generator_\" + str(epoch) + \".h5\")\n",
        "  discriminator.save(model_save_filepath + \"discriminator_\" + str(epoch) + \".h5\")\n",
        "\n",
        "  generator.save_weights(checkpoint_filepath + \"generator_weights_\" + str(epoch) + \".h5\")\n",
        "  discriminator.save_weights(checkpoint_filepath + \"discriminator_weights_\" + str(epoch) + \".h5\")"
      ],
      "metadata": {
        "id": "PYq6xPlKhAF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/fyp-datasets/raw-890/*'\n",
        "image_shape = (256, 256, 3)"
      ],
      "metadata": {
        "id": "NdjjLIAfdOpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_enhanced_images, all_original_images = load_all_images(data_dir, (image_shape[0], image_shape[1]))\n",
        "print(len(all_enhanced_images), len(all_original_images))"
      ],
      "metadata": {
        "id": "COpOsTQfvC10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the below values as necessary\n",
        "epochs = 7880\n",
        "batch_size = 8\n",
        "\n",
        "loss_step = 100 # Interval to write losses to file\n",
        "display_step = 100 # Interval to display sample raw-reference-generated images\n",
        "checkpoint_step = 1000 # Interval at which to take checkpoint\n",
        "\n",
        "continue_training = True # Set False if starting from epoch 0, else True\n",
        "gen_weights_file = 'generator_weights_6000.h5' # Change file name based on where to continue from\n",
        "disc_weights_file = 'discriminator_weights_6000.h5'\n",
        "losses_file = 'losses.txt' # File to save losses to (IMPORTANT: for plotting later)"
      ],
      "metadata": {
        "id": "WL63HZ9Lp1PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Common optimizer for all networks\n",
        "common_optimizer = tf.keras.optimizers.Adam(0.0002, 0.5)\n",
        "\n",
        "# Build and compile VGG19 network to extract features\n",
        "vgg = build_vgg()\n",
        "vgg.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Build and compile the discriminator network\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Build the generator network\n",
        "generator = build_generator()\n",
        "\n",
        "# Input layers for original and enhanced images\n",
        "input_original = Input(shape=image_shape)\n",
        "input_enhanced = Input(shape=image_shape)\n",
        "\n",
        "# Generate enhanced images from original images\n",
        "generated_enhanced_images = generator(input_original)\n",
        "\n",
        "# Extract feature maps of the generated images\n",
        "features = vgg(generated_enhanced_images)\n",
        "\n",
        "# Make the discriminator network as non-trainable\n",
        "discriminator.trainable = False\n",
        "\n",
        "# Get the probability of generated enhanced images\n",
        "probs = discriminator(generated_enhanced_images)\n",
        "\n",
        "# Create and compile an adversarial model\n",
        "adversarial_model = Model([input_original, input_enhanced], [probs, features])\n",
        "adversarial_model.compile(loss=['binary_crossentropy', 'mse'], loss_weights=[1e-3, 1], optimizer=common_optimizer)\n",
        "\n",
        "# For calculating FID\n",
        "inception_model = InceptionV3(include_top=False, pooling='avg', input_shape=(256,256,3))"
      ],
      "metadata": {
        "id": "_rajyntOJP0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment to see Generator architecture\n",
        "# tf.keras.utils.plot_model(generator,show_shapes=True)"
      ],
      "metadata": {
        "id": "ezzoDh__g5mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment to see Discriminator architecture\n",
        "# tf.keras.utils.plot_model(discriminator,show_shapes=True)"
      ],
      "metadata": {
        "id": "VYBAhkxlg6jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = 1 # Don't change this, default value\n",
        "losses_file_obj = ''\n",
        "\n",
        "# If continuing training from checkpoint\n",
        "if continue_training:\n",
        "  start = 6001 # Change based on where to continue from\n",
        "  generator.load_weights(checkpoint_filepath + gen_weights_file)\n",
        "  discriminator.load_weights(checkpoint_filepath + disc_weights_file)\n",
        "  # Open losses text file in append mode\n",
        "  losses_file_obj = open(checkpoint_filepath + losses_file, 'a')\n",
        "else:\n",
        "  losses_file_obj = open(checkpoint_filepath + losses_file, 'w+')"
      ],
      "metadata": {
        "id": "NVfpfIeYfM-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  for epoch in range(start, epochs + 1):\n",
        "      # Sample a batch of images\n",
        "      original_images, enhanced_images = select_random_batch(all_enhanced_images, all_original_images, batch_size)\n",
        "      \n",
        "      # Normalizing\n",
        "      original_images = original_images / 127.5 - 1.\n",
        "      enhanced_images = enhanced_images / 127.5 - 1.\n",
        "\n",
        "      # Generate enhanced images (fake) from original images (real)\n",
        "      generated_enhanced_images = generator.predict(original_images)\n",
        "\n",
        "      # Generate batch of real and fake labels\n",
        "      real_labels = np.ones((batch_size, 16, 16, 1))\n",
        "      fake_labels = np.zeros((batch_size, 16, 16, 1))\n",
        "\n",
        "      # Train the discriminator network on real and fake images\n",
        "      d_loss_real = discriminator.train_on_batch(enhanced_images, real_labels)\n",
        "      d_loss_fake = discriminator.train_on_batch(generated_enhanced_images, fake_labels)\n",
        "\n",
        "      # Calculate total discriminator loss\n",
        "      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "      # TRAINING GENERATOR NETWORK\n",
        "      original_images, enhanced_images = select_random_batch(all_original_images, all_enhanced_images, batch_size)\n",
        "\n",
        "      # Normalizing\n",
        "      original_images = original_images / 127.5 - 1.\n",
        "      enhanced_images = enhanced_images / 127.5 - 1.\n",
        "\n",
        "      # Extract feature maps for enhanced images\n",
        "      image_features = vgg.predict(enhanced_images)\n",
        "\n",
        "      # Train the generator network\n",
        "      g_loss = adversarial_model.train_on_batch([original_images, enhanced_images],\n",
        "                                        [real_labels, image_features])\n",
        "\n",
        "      if epoch % loss_step == 0:\n",
        "        print(\"Epoch:{}\".format(epoch))\n",
        "        print(\"d_loss:\", d_loss)\n",
        "        print(\"g_loss:\", g_loss)\n",
        "\n",
        "        loss_to_write = \"{epoch} {d_loss} {g_loss_per}\\n\".format(epoch=epoch, d_loss=d_loss[0], g_loss_per=g_loss[0])\n",
        "        losses_file_obj.write(loss_to_write)\n",
        "\n",
        "      if epoch != 1 and epoch % display_step == 0:\n",
        "        original_images, enhanced_images = select_random_batch(all_original_images, all_enhanced_images, batch_size)\n",
        "\n",
        "        # Normalizing\n",
        "        original_images = original_images / 127.5 - 1.\n",
        "        enhanced_images = enhanced_images / 127.5 - 1.\n",
        "\n",
        "        generated_images = generator.predict_on_batch(original_images)\n",
        "\n",
        "        fid = calculate_fid(inception_model, enhanced_images, generated_images)\n",
        "        print('FID: %.3f' % fid)\n",
        "\n",
        "        fid_to_write = \"FID {epoch} {FID}\\n\".format(epoch=epoch, FID=fid)\n",
        "        losses_file_obj.write(fid_to_write)\n",
        "\n",
        "        for index, img in enumerate(generated_images):\n",
        "          if index >= 4:\n",
        "            break\n",
        "          display_images(original_images[index], enhanced_images[index], img)\n",
        "      \n",
        "      if epoch != 1 and epoch % checkpoint_step == 0:\n",
        "        take_checkpoint(generator, discriminator, checkpoint_filepath, epoch)\n",
        "finally:\n",
        "  losses_file_obj.close()\n",
        "\n",
        "# Once training is completed for all epochs, final checkpoint\n",
        "take_checkpoint(generator, discriminator, checkpoint_filepath, epoch)"
      ],
      "metadata": {
        "id": "xQLyS8yyqld1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing losses\n",
        "---"
      ],
      "metadata": {
        "id": "I6-p0ecoeKeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "losses_file_to_plot = open(checkpoint_filepath + losses_file, 'r')\n",
        "gen_loss_plot = []\n",
        "disc_loss_plot = []\n",
        "loss_epochs = []\n",
        "FID_plot = []\n",
        "FID_epochs = []\n",
        "\n",
        "lines = losses_file_to_plot.readlines()\n",
        "\n",
        "for line in lines:\n",
        "  values = line.split()\n",
        "  if values[0] == \"FID\":\n",
        "    FID_epochs.append(int(values[1]))\n",
        "    FID_plot.append(float(values[2]))\n",
        "  else:\n",
        "    loss_epochs.append(int(values[0]))\n",
        "    disc_loss_plot.append(float(values[1]))\n",
        "    gen_loss_plot.append(float(values[2]))\n",
        "\n",
        "losses_file_to_plot.close()"
      ],
      "metadata": {
        "id": "Z0Eeb2-gdkDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot generator loss\n",
        "plot_losses(gen_loss_plot, loss_epochs, 'Generator loss')"
      ],
      "metadata": {
        "id": "JN6JunI25fHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot discriminator loss\n",
        "plot_losses(disc_loss_plot, loss_epochs, 'Discriminator loss')"
      ],
      "metadata": {
        "id": "hBGgESKZ6Q0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot FID\n",
        "plot_losses(FID_plot, FID_epochs, 'FID')"
      ],
      "metadata": {
        "id": "J1t4aAk1fqr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n",
        "---\n"
      ],
      "metadata": {
        "id": "B98FA-YCPKkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a saved model - change filename based on which model to load\n",
        "generator = tf.keras.models.load_model(model_save_filepath + \"generator_2000.h5\", compile=False)"
      ],
      "metadata": {
        "id": "VaomT5KKPJ5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting predictions to calculate FID - change number of images in batch as necessary\n",
        "original_images, enhanced_images = select_random_batch(all_original_images, all_enhanced_images, 1000)\n",
        "enhanced_images = enhanced_images / 127.5 - 1.\n",
        "original_images = original_images / 127.5 - 1.\n",
        "generated_images = []\n",
        "for image in original_images:\n",
        "  exp = tf.expand_dims(image, axis=0)\n",
        "  temp = generator.predict(exp)\n",
        "  generated_images.append(temp[0])\n",
        "generated_images = np.array(generated_images)"
      ],
      "metadata": {
        "id": "u2lIvitumm_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FID calculation\n",
        "inception_model = InceptionV3(include_top=False, pooling='avg', input_shape=(256,256,3))\n",
        "fid = calculate_fid(inception_model, enhanced_images, generated_images)\n",
        "print('FID: %.3f' % fid)"
      ],
      "metadata": {
        "id": "52Oav27An_43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing on UIEB \"challenging\" set (without reference images) - change image shape as necessary\n",
        "data_dir = '/content/fyp-datasets/challenging-60/*'\n",
        "test_images = load_test_images(data_dir=data_dir, batch_size=2, image_shape=(400,350))\n",
        "test_images = test_images / 127.5 - 1.\n",
        "\n",
        "generated_images = generator.predict_on_batch(test_images)\n",
        "for index, img in enumerate(generated_images):\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(1, 2, 1)\n",
        "  ax.imshow(test_images[index])\n",
        "  ax.axis(\"off\")\n",
        "  ax.set_title(\"Test Image\")\n",
        "\n",
        "  ax = fig.add_subplot(1, 2, 2)\n",
        "  ax.imshow(img)\n",
        "  ax.axis(\"off\")\n",
        "  ax.set_title(\"Generated Image\")\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "M_93O66Ffw55"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}